# Multimodal tool
"Multimodal" is one of the areas in AI research, it means one task needs more than 1 AI modal to achieve the goal.  
For example, our data target [VisualComet](https://visualcomet.xyz/) uses image recognization and Natural Language Processing (NLP) to predict the past, present, and future for one image.  
In other words, researchers often need to deal with results in multiple formats and that would cause a lot of inconveniences.  
Since they don't have a proper tool, they have to compare raw JSONs with the image by themselves.  
Therefore, the goal of this project is providing a tool for researchers to work on multimodal datasets comfortably.  
## Features
- Two modes:  
Exploration mode allows users to skim through, group data, and dive into details of data instances as needed.  
Comparison mode allows users to compare results side by side and relate between modalities.  
- Grouping function: Filter out a subset of data to explore properties of the dataset at the group level.  
- Scatter plot: Group comparisons that allow customization on the X and Y axis.  

## Screenshots
List in Exploration mode  
![exploration list](https://i.imgur.com/HWlyZTZ.png)  

A single instance in Exploration mode, this page only shows the ground truth
![exploration single](https://i.imgur.com/zEVwI9E.png)

List in Comparison mode
![comparison list](https://i.imgur.com/lHyJDuk.png)  

A single instance in Comparison mode, researchers can compare the ground truth with the result generated by AI
![comparison single](https://i.imgur.com/AvgXp1w.png)  
## Languages
JavaScript  
HTML/CSS  
Python
## Frameworks/Libraries
React  
Express  
React Bounding Box Component  
Reach UI  
Styled-JSX  
D3  
## Credits
This tool was made in collaboration with [Pin-Cheng Chen](https://github.com/gilesabc), [Yiming Li](https://github.com/Lucas0193), and [Zijiao Yang](https://github.com/Yoark) in CS 539 Data Visualization at Oregon State University.

## Getting start
1. Clone this repository by running
```
$ git clone https://github.com/Yoark/multimodal-tool.git
```

2. Get into the root folder `/` by `$ cd multimodal-tool` and then run `$ npm install` to install all necessary libraries.  
If you don't have such command, you need to install Node.  
I recommand [this video](https://www.youtube.com/watch?v=9hb_0TZ_MVI&list=PLC3y8-rFHvwgg3vaYJgHGnModB54rxOk3&index=2), I feel it's pretty clear.  

3. Put the original data into the `/public/data` folder, notice that the folder name should be lower case.  
```
public
├── data
│   ├── 100_sample_from_val_predicted.json
│   ├── 100_val-ground.json
│   └── image_val_100
│       └── image folders...
└── index.html...
```

4. Run `$ npm start` in both `/` and `/frontend`, then open your [localhost:3000](http://localhost:3000/) to check the work.
